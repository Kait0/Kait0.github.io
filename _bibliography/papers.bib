---
---

@mastersthesis{jaeger2021expertdrivers,
  bibtex_show={true},
  title={Expert Drivers for Autonomous Driving},
  author={Jaeger, Bernhard},
  year={2021},
  selected={true},
  abstract={A popular approach to self-driving is imitation learning, in which a neural network is trained to predict the actions of an expert driver for a given input. For research performed on simulators, these expert drivers are usually computer programs that have privileged access to the simulation. While these approaches are more cost-efficient than using labels from humans, their quality is worse because existing expert driver approaches are not able to safely drive through urban environments. We improve upon prior work and propose SEED, a Simple and Effective Expert Driver that can safely navigate through urban environments even under challenging adversarial traffic scenarios. We show its utility by training an existing imitation learning architecture to imitate SEED. The resulting TransFuser+ method sets a new state-of-the-art on the challenging NEAT validation routes and outperforms the best prior work on the CARLA leaderboard.},
  pdf={master_thesis_bernhard_jaeger.pdf},
  school={University of TÃ¼bingen},
  teaser={master_thesis_teaser.png},
}

@article{Chitta2022TPAMI,
  bibtex_show={true},
  selected={true},
  author = {Chitta, Kashyap and
            Prakash, Aditya and
            Jaeger, Bernhard and
            Yu, Zehao and
            Renz, Katrin and
            Geiger, Andreas},
  title = {TransFuser: Imitation with Transformer-Based Sensor Fusion for Autonomous Driving},
  journal = {IEEE Transactions on Pattern Analysis &amp; Machine Intelligence},
  doi = {10.1109/TPAMI.2022.3200245},
  year = {2022},
  pdf={https://arxiv.org/abs/2205.15997},
  abstract = {How should we integrate representations from complementary sensors for autonomous driving? Geometry-based fusion has shown promise for perception (e.g. object detection, motion forecasting). However, in the context of end-to-end driving, we find that imitation learning based on existing sensor fusion methods underperforms in complex driving scenarios with a high density of dynamic agents. Therefore, we propose TransFuser, a mechanism to integrate image and LiDAR representations using self-attention. Our approach uses transformer modules at multiple resolutions to fuse perspective view and bird&amp;#x0027;s eye view feature maps. We experimentally validate its efficacy on a challenging new benchmark with long routes and dense traffic, as well as the official leaderboard of the CARLA urban driving simulator. At the time of submission, TransFuser outperforms all prior work on the CARLA leaderboard in terms of driving score by a large margin. Compared to geometry-based fusion, TransFuser reduces the average collisions per kilometer by 48&amp;#x0025;.},
  publisher = {IEEE Computer Society},
  teaser={TransFuserTeaser.PNG}
}